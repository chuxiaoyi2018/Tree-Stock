{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a664db11-1b6b-4a86-a2bc-5564c25f0669",
   "metadata": {},
   "outputs": [],
   "source": [
    "from get_stock_data import Downloader, mkdir\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import catboost as cb\n",
    "import datetime\n",
    "import functools\n",
    "from collections import defaultdict, Counter\n",
    "from sklearn import preprocessing\n",
    "\n",
    "from utils import reduce_mem, StockDNN\n",
    "\n",
    "import torch\n",
    "from torchmetrics import AUROC, Recall, Accuracy, F1Score\n",
    "\n",
    "# 获取全部股票的日K线数据\n",
    "mkdir('stockdata/d_data')\n",
    "raw_train_path = 'stockdata/d_train'\n",
    "raw_test_path = 'stockdata/d_test'\n",
    "train_path = 'stockdata/d_data/train.csv'\n",
    "test_path = 'stockdata/d_data/test.csv'\n",
    "industry_path = 'stockdata/stock_industry.csv'\n",
    "mode = 'debug'\n",
    "\n",
    "\n",
    "concept_path = 'stockdata/concept_df.csv'\n",
    "concept_hist_path = 'stockdata/concept_hist_df.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "84a12b3c-3c10-439f-9348-ceed20266795",
   "metadata": {},
   "outputs": [],
   "source": [
    "if mode == 'debug':\n",
    "    train = pd.read_csv(train_path, nrows=100000)\n",
    "    test = pd.read_csv(test_path, nrows=100000)\n",
    "else:\n",
    "    train = pd.read_csv(train_path)\n",
    "    test = pd.read_csv(test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "52643e75-12da-49c2-8a98-1d31e011b493",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.date = train.date.apply(lambda x: int(x.replace('-', '')))\n",
    "test.date = test.date.apply(lambda x: int(x.replace('-', '')))\n",
    "\n",
    "train.code = train.code.apply(lambda x:x[3:])\n",
    "test.code = test.code.apply(lambda x:x[3:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ad77c9c-6598-405a-8ebd-326412bfdffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train[train['date'] >= 20210101].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7a2e5fec-ffd5-4556-8c87-17aa6f5d353b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def concept_feature(data):\n",
    "    print('Begin concept feature')\n",
    "    concept_df = pd.read_csv(concept_path)[['代码', '板块名称']]\n",
    "    concept_hist_df = pd.read_csv(concept_hist_path)\n",
    "    concept_df['代码'] = concept_df['代码'].apply(lambda x:str(x).zfill(6))\n",
    "    \n",
    "    concept_hist_df['日期'] = concept_hist_df['日期'].apply(lambda x:int(x.replace('-', '')))\n",
    "    \n",
    "    concept_counter = Counter([c for c in concept_df['板块名称'].values if '昨日' not in c])\n",
    "\n",
    "    concept_dic = defaultdict(list)\n",
    "    for code, concept in concept_df.values:\n",
    "        if '昨日' in concept:continue\n",
    "        concept_dic[code].append(concept)\n",
    "\n",
    "    def compare_concept(x, y):\n",
    "        x, y = concept_counter[x], concept_counter[y]\n",
    "        if x < y:return -1\n",
    "        if x > y: return 1\n",
    "        return 0\n",
    "    for k, v in concept_dic.items():\n",
    "        concept_dic[k] = sorted(v, key=functools.cmp_to_key(compare_concept))\n",
    "\n",
    "    data['concept_0'] = data.code.apply(lambda x:concept_dic[x][0] if len(concept_dic[x])>0 else np.nan)\n",
    "    data['concept_1'] = data.code.apply(lambda x:concept_dic[x][1] if len(concept_dic[x])>1 else np.nan)\n",
    "    data['concept_2'] = data.code.apply(lambda x:concept_dic[x][2] if len(concept_dic[x])>2 else np.nan)\n",
    "\n",
    "    data['concept_-3'] = data.code.apply(lambda x:concept_dic[x][-3] if len(concept_dic[x])>3 else np.nan)\n",
    "    data['concept_-2'] = data.code.apply(lambda x:concept_dic[x][-2] if len(concept_dic[x])>4 else np.nan)\n",
    "    data['concept_-1'] = data.code.apply(lambda x:concept_dic[x][-1] if len(concept_dic[x])>5 else np.nan)\n",
    "    \n",
    "    data = pd.merge(data, concept_hist_df, left_on=['date', 'concept_0'], right_on=['日期', '板块名称'], how='left')\n",
    "    data = pd.merge(data, concept_hist_df, left_on=['date', 'concept_1'], right_on=['日期', '板块名称'], how='left', suffixes=(None, '_1'))\n",
    "    data = pd.merge(data, concept_hist_df, left_on=['date', 'concept_2'], right_on=['日期', '板块名称'], how='left', suffixes=(None, '_2'))\n",
    "    data = pd.merge(data, concept_hist_df, left_on=['date', 'concept_-3'], right_on=['日期', '板块名称'], how='left', suffixes=(None, '_-3'))\n",
    "    data = pd.merge(data, concept_hist_df, left_on=['date', 'concept_-2'], right_on=['日期', '板块名称'], how='left', suffixes=(None, '_-2'))\n",
    "    data = pd.merge(data, concept_hist_df, left_on=['date', 'concept_-1'], right_on=['日期', '板块名称'], how='left', suffixes=(None, '_-1'))\n",
    "    \n",
    "    \n",
    "    # label encoder\n",
    "    concept_labelencoder = {c:i for i, c in enumerate(np.unique(concept_df['板块名称'].values))}\n",
    "    concept_labelencoder.update({np.nan:np.nan})\n",
    "    data['concept_0'] = data['concept_0'].apply(lambda x:concept_labelencoder[x])\n",
    "    data['concept_1'] = data['concept_1'].apply(lambda x:concept_labelencoder[x])\n",
    "    data['concept_2'] = data['concept_2'].apply(lambda x:concept_labelencoder[x])\n",
    "    \n",
    "    data['concept_-3'] = data['concept_-3'].apply(lambda x:concept_labelencoder[x])\n",
    "    data['concept_-2'] = data['concept_-2'].apply(lambda x:concept_labelencoder[x])\n",
    "    data['concept_-1'] = data['concept_-1'].apply(lambda x:concept_labelencoder[x])\n",
    "    return data\n",
    "\n",
    "def feature_engineer(train, test, split=20220501):\n",
    "    train_len = len(train)\n",
    "    data = pd.concat((train, test), sort=False).reset_index(drop=True)\n",
    "    data = data.sort_values(by=['code', 'date'])\n",
    "    \n",
    "    stock_industry = pd.read_csv(industry_path, encoding=\"gbk\")\n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "    lbe = LabelEncoder()\n",
    "    stock_industry['industry'] = lbe.fit_transform(stock_industry['industry'])\n",
    "    data = pd.merge(data, stock_industry[['code', 'industry']], how='left', on='code')\n",
    "    \n",
    "    # concept feature\n",
    "    data = concept_feature(data)\n",
    "    data = reduce_mem(data, list(data))\n",
    "\n",
    "    # alpha net \n",
    "    length = 30\n",
    "    alpha_list = ['open', 'high', 'low', 'close', 'volume', 'amount', 'adjustflag', 'turn', 'pctChg', 'peTTM', 'psTTM', 'pcfNcfTTM', 'pbMRQ']\n",
    "    alpha_list += [f'{x}{i}' for x in ['收盘', '换手率', '成交额'] for i in ['', '_1', '_2', '_-3', '_-2', '_-1']]\n",
    "    for name in tqdm(alpha_list):\n",
    "#     for name in tqdm(['open']):\n",
    "        roll_feature = []\n",
    "        for i, group in data.groupby('code', sort=False)[name]:\n",
    "            values = group.tolist()\n",
    "            values = [0]*(length - 1) + values\n",
    "            roll_feature = roll_feature + [values[i:i+length] for i in range(len(group))]\n",
    "        roll_columns = [f'{name}_dt{i}' for i in range(length)]\n",
    "        data = pd.concat([data, pd.DataFrame(roll_feature, columns=roll_columns)], axis=1).reset_index(drop=True)\n",
    "        data = reduce_mem(data, roll_columns)\n",
    "    \n",
    "    # generate label\n",
    "    data['label'] = data.groupby('code', sort=False).close.transform(lambda x:(x.shift(-14) - x) / (x + 1e-7) )\n",
    "    data = data.dropna(subset = ['label'], inplace=False)\n",
    "    data = data.replace(np.nan, 0)\n",
    "    return data[data['date'] <= split].reset_index(drop=True), data[data['date'] > split].reset_index(drop=True)\n",
    "\n",
    "def minmax_scaler(train, test, feature_names):\n",
    "    for name in feature_names:\n",
    "#         if 'concept' in name:continue\n",
    "        max_value = train[name].max()\n",
    "        min_value = train[name].min()\n",
    "        train[name] = (train[name] - min_value)/(1e-7 + max_value - min_value)\n",
    "        test[name] = (test[name] - min_value)/(1e-7 + max_value - min_value)\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5eca384c-84a6-42d8-9733-3d18d4fe7773",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin concept feature\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/31 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83.58 Mb, 31.55 Mb (62.24 %)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 1/31 [00:03<01:39,  3.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56.29 Mb, 37.10 Mb (34.09 %)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▋         | 2/31 [00:06<01:36,  3.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62.68 Mb, 43.49 Mb (30.61 %)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|▉         | 3/31 [00:10<01:34,  3.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69.08 Mb, 49.89 Mb (27.78 %)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 4/31 [00:13<01:31,  3.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75.47 Mb, 56.29 Mb (25.42 %)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 5/31 [00:16<01:28,  3.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "81.87 Mb, 69.08 Mb (15.62 %)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 6/31 [00:20<01:24,  3.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94.66 Mb, 81.87 Mb (13.51 %)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 7/31 [00:23<01:22,  3.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "107.46 Mb, 85.07 Mb (20.83 %)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 8/31 [00:27<01:19,  3.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "110.65 Mb, 91.47 Mb (17.34 %)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▉       | 9/31 [00:30<01:15,  3.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "117.05 Mb, 97.86 Mb (16.39 %)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 10/31 [00:34<01:12,  3.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "123.45 Mb, 108.95 Mb (11.74 %)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 11/31 [00:37<01:08,  3.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "134.53 Mb, 120.03 Mb (10.78 %)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|███▊      | 12/31 [00:41<01:05,  3.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "145.62 Mb, 131.12 Mb (9.96 %)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 13/31 [00:44<01:02,  3.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "156.71 Mb, 137.52 Mb (12.24 %)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 14/31 [00:48<00:59,  3.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "163.10 Mb, 143.91 Mb (11.76 %)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 15/31 [00:51<00:56,  3.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "169.50 Mb, 150.31 Mb (11.32 %)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 16/31 [00:55<00:52,  3.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "175.89 Mb, 156.71 Mb (10.91 %)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▍    | 17/31 [00:58<00:49,  3.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "182.29 Mb, 163.10 Mb (10.53 %)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 18/31 [01:02<00:46,  3.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "188.69 Mb, 169.50 Mb (10.17 %)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|██████▏   | 19/31 [01:06<00:43,  3.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "195.08 Mb, 175.89 Mb (9.84 %)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▍   | 20/31 [01:09<00:39,  3.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "201.48 Mb, 182.29 Mb (9.52 %)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 21/31 [01:13<00:36,  3.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "207.88 Mb, 188.69 Mb (9.23 %)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████   | 22/31 [01:17<00:33,  3.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "214.27 Mb, 195.08 Mb (8.96 %)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|███████▍  | 23/31 [01:21<00:29,  3.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "220.67 Mb, 201.48 Mb (8.70 %)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|███████▋  | 24/31 [01:25<00:26,  3.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "227.06 Mb, 207.88 Mb (8.45 %)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|████████  | 25/31 [01:28<00:22,  3.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "233.46 Mb, 214.27 Mb (8.22 %)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████▍ | 26/31 [01:32<00:18,  3.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "239.86 Mb, 227.06 Mb (5.33 %)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 27/31 [01:36<00:15,  3.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "252.65 Mb, 239.86 Mb (5.06 %)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 28/31 [01:40<00:11,  3.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "265.44 Mb, 252.65 Mb (4.82 %)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▎| 29/31 [01:43<00:07,  3.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "278.23 Mb, 265.44 Mb (4.60 %)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|█████████▋| 30/31 [01:47<00:03,  3.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "291.03 Mb, 278.23 Mb (4.40 %)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 31/31 [01:51<00:00,  3.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "303.82 Mb, 291.03 Mb (4.21 %)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train, test = feature_engineer(train, test)\n",
    "\n",
    "ycol = 'label'\n",
    "feature_names = list(\n",
    "    filter(lambda x: x not in [ycol, 'code', 'date', ''] and '日期' not in x and '板块名称' not in x, train.columns))\n",
    "\n",
    "# print(feature_names)\n",
    "\n",
    "train, test = minmax_scaler(train, test, feature_names)\n",
    "\n",
    "# f_train_path = 'stockdata/d_data/f_train_debug.csv'\n",
    "# f_test_path = 'stockdata/d_data/f_test_debug.csv'\n",
    "# train.to_csv(f_train_path, index=False)\n",
    "# test.to_csv(f_test_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e9b44194-86e7-4cd8-bdc8-025cbe16f898",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "quantile_30: -0.048828125\n",
      "quantile_70: 0.03344726562499997\n"
     ]
    }
   ],
   "source": [
    "quantile_30, quantile_70 = train.label.quantile([0.3, 0.7]).values\n",
    "print('quantile_30:', quantile_30)\n",
    "print('quantile_70:', quantile_70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e64a0232-75dc-4375-83be-ab5e239bfc0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_quantile(x):\n",
    "    if x<quantile_30:\n",
    "        return 0\n",
    "    elif x<quantile_70:\n",
    "        return 1\n",
    "    else:\n",
    "        return 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1cbcb7b6-ddfb-4b9b-b45a-f8131adb8a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.label = train.label.apply(label_quantile)\n",
    "test.label = test.label.apply(label_quantile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "01221f4f-e445-4556-b2a2-1ecb265e706e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp_train(train, test, feature_names, ycol):\n",
    "    epochs = 20\n",
    "    batch_size = 512\n",
    "    input_dim = len(feature_names)\n",
    "    \n",
    "#     device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "    device = torch.device('cpu')\n",
    "    model = StockDNN(input_dim=input_dim, output_dim=3).to(device)\n",
    "    optimizer = torch.optim.Adam(filter(lambda p:p.requires_grad, model.parameters()), lr=3e-4)\n",
    "    print(model)\n",
    "    \n",
    "    X_train = torch.Tensor(train[feature_names].to_numpy())\n",
    "    Y_train = torch.Tensor(train[ycol].to_numpy()).long()\n",
    "    X_val = torch.Tensor(test[feature_names].to_numpy())\n",
    "    Y_val = torch.Tensor(test[ycol].to_numpy()).long()\n",
    "    n_samples = len(X_train)\n",
    "    iterations = n_samples // batch_size\n",
    "    idx = np.arange(n_samples)\n",
    "    \n",
    "    \n",
    "    for _ in range(epochs):\n",
    "        for i in range(0, n_samples, batch_size):\n",
    "            batch_idx = idx[i:i+batch_size]\n",
    "            batch_data = X_train[batch_idx]\n",
    "            batch_target = Y_train[batch_idx]\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss = model.compute_loss(batch_data.to(device), batch_target.to(device))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "                \n",
    "\n",
    "        np.random.shuffle(idx)\n",
    "        torch.save(model, 'model/concept_mlp.pt')\n",
    "        \n",
    "        # metrics\n",
    "        preds = model(X_val)\n",
    "        auroc = AUROC(num_classes=3)\n",
    "        accuracy = Accuracy(nums_classes=3)\n",
    "        f1 = F1Score(num_classes=3, threshold=0.5)\n",
    "        recall = Recall(num_classes=3, threshold=0.5)\n",
    "        print('AUROC:', auroc(preds, Y_val), ' | ', 'Accuracy:', accuracy(preds, Y_val), '|', 'Recall:', recall(preds, Y_val), '|', 'F1Score:', f1(preds, Y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ddfa92d2-e4a2-4ae2-9d2a-d03eff901335",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StockDNN(\n",
      "  (loss_func): CrossEntropyLoss()\n",
      "  (mlp): Sequential(\n",
      "    (0): Linear(in_features=1013, out_features=1024, bias=True)\n",
      "    (1): Dropout(p=0.5, inplace=False)\n",
      "    (2): LeakyReLU(negative_slope=0.2)\n",
      "    (3): Linear(in_features=1024, out_features=512, bias=True)\n",
      "    (4): Dropout(p=0.5, inplace=False)\n",
      "    (5): LeakyReLU(negative_slope=0.2)\n",
      "    (6): Linear(in_features=512, out_features=256, bias=True)\n",
      "    (7): Dropout(p=0.5, inplace=False)\n",
      "    (8): LeakyReLU(negative_slope=0.2)\n",
      "    (9): Linear(in_features=256, out_features=3, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.7/site-packages/torchmetrics/utilities/prints.py:36: UserWarning: Metric `AUROC` will save all targets and predictions in buffer. For large datasets this may lead to large memory footprint.\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUROC: tensor(0.5066)  |  Accuracy: tensor(0.4138) | Recall: tensor(0.4138) | F1Score: tensor(0.4138)\n",
      "AUROC: tensor(0.5217)  |  Accuracy: tensor(0.4097) | Recall: tensor(0.4097) | F1Score: tensor(0.4097)\n",
      "AUROC: tensor(0.5300)  |  Accuracy: tensor(0.3924) | Recall: tensor(0.3924) | F1Score: tensor(0.3924)\n",
      "AUROC: tensor(0.5246)  |  Accuracy: tensor(0.2956) | Recall: tensor(0.2956) | F1Score: tensor(0.2956)\n",
      "AUROC: tensor(0.5208)  |  Accuracy: tensor(0.2269) | Recall: tensor(0.2269) | F1Score: tensor(0.2269)\n",
      "AUROC: tensor(0.5216)  |  Accuracy: tensor(0.2351) | Recall: tensor(0.2351) | F1Score: tensor(0.2351)\n",
      "AUROC: tensor(0.5221)  |  Accuracy: tensor(0.2228) | Recall: tensor(0.2228) | F1Score: tensor(0.2228)\n",
      "AUROC: tensor(0.5254)  |  Accuracy: tensor(0.2264) | Recall: tensor(0.2264) | F1Score: tensor(0.2264)\n",
      "AUROC: tensor(0.5257)  |  Accuracy: tensor(0.2478) | Recall: tensor(0.2478) | F1Score: tensor(0.2478)\n",
      "AUROC: tensor(0.5261)  |  Accuracy: tensor(0.2582) | Recall: tensor(0.2582) | F1Score: tensor(0.2582)\n",
      "AUROC: tensor(0.5267)  |  Accuracy: tensor(0.2256) | Recall: tensor(0.2256) | F1Score: tensor(0.2256)\n",
      "AUROC: tensor(0.5275)  |  Accuracy: tensor(0.2902) | Recall: tensor(0.2902) | F1Score: tensor(0.2902)\n",
      "AUROC: tensor(0.5269)  |  Accuracy: tensor(0.2326) | Recall: tensor(0.2326) | F1Score: tensor(0.2326)\n",
      "AUROC: tensor(0.5254)  |  Accuracy: tensor(0.2650) | Recall: tensor(0.2650) | F1Score: tensor(0.2650)\n",
      "AUROC: tensor(0.5261)  |  Accuracy: tensor(0.2635) | Recall: tensor(0.2635) | F1Score: tensor(0.2635)\n",
      "AUROC: tensor(0.5259)  |  Accuracy: tensor(0.2709) | Recall: tensor(0.2709) | F1Score: tensor(0.2709)\n",
      "AUROC: tensor(0.5241)  |  Accuracy: tensor(0.2960) | Recall: tensor(0.2960) | F1Score: tensor(0.2960)\n",
      "AUROC: tensor(0.5249)  |  Accuracy: tensor(0.2565) | Recall: tensor(0.2565) | F1Score: tensor(0.2565)\n",
      "AUROC: tensor(0.5285)  |  Accuracy: tensor(0.2720) | Recall: tensor(0.2720) | F1Score: tensor(0.2720)\n",
      "AUROC: tensor(0.5264)  |  Accuracy: tensor(0.2522) | Recall: tensor(0.2522) | F1Score: tensor(0.2522)\n"
     ]
    }
   ],
   "source": [
    "mlp_train(train, test, feature_names, ycol)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
